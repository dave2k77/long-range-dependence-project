{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df204d10-54a9-4649-9dfb-15b0743592a7",
   "metadata": {},
   "source": [
    "## Pure Signal Generators\n",
    "### ARFIMA Processes\n",
    "\n",
    "ARFIMA (Autoregressive Fractionally Integrated Moving Average) processes are characterized by the fractional differencing parameter `d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40606949-7af2-470f-84b4-850d8cf8bc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\davia\\anaconda3\\envs\\fractal-analysis\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured directory exists: data\n",
      "Ensured directory exists: data\\raw\n",
      "Ensured directory exists: data\\processed\n",
      "Ensured directory exists: data\\metadata\n",
      "Weak LRD (d=0.1): 1000 points\n",
      "Medium LRD (d=0.3): 1000 points\n",
      "Strong LRD (d=0.4): 1000 points\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "from data_processing.synthetic_generator import SyntheticDataGenerator\n",
    "\n",
    "# Initialize generator\n",
    "generator = SyntheticDataGenerator(random_state=42)\n",
    "\n",
    "# Generate ARFIMA with different d values using the new convenience method\n",
    "arfima_weak = generator.generate_arfima(n=1000, d=0.1)  # Weak LRD\n",
    "arfima_medium = generator.generate_arfima(n=1000, d=0.3)  # Medium LRD\n",
    "arfima_strong = generator.generate_arfima(n=1000, d=0.4)  # Strong LRD\n",
    "\n",
    "print(f\"Weak LRD (d=0.1): {len(arfima_weak)} points\")\n",
    "print(f\"Medium LRD (d=0.3): {len(arfima_medium)} points\")\n",
    "print(f\"Strong LRD (d=0.4): {len(arfima_strong)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273308a7-bb9f-4cb8-8b3d-2f101603613a",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `n`: Number of data points\n",
    "- `d`: Fractional differencing parameter (0 < d < 0.5)\n",
    "- `ar_params`: Optional AR parameters (list of floats)\n",
    "- `ma_params`: Optional MA parameters (list of floats)\n",
    "- `sigma`: Noise standard deviation (default: 1.0)\n",
    "- `random_state`: For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a14acb-49c1-489f-a753-1a10e4e8d33e",
   "metadata": {},
   "source": [
    "### Fractional Brownian Motion (fBm)\n",
    "\n",
    "fBm is a generalization of Brownian motion with Hurst exponent H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af250d1-e9b5-4691-810f-b7c3489d833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti-persistent fBm (H=0.3): 1000 points\n",
      "Random walk fBm (H=0.5): 1000 points\n",
      "Persistent fBm (H=0.7): 1000 points\n"
     ]
    }
   ],
   "source": [
    "# Generate fBm with different Hurst exponents using the new convenience method\n",
    "fbm_anti = generator.generate_fbm(n=1000, hurst=0.3)  # Anti-persistent\n",
    "fbm_random = generator.generate_fbm(n=1000, hurst=0.5)  # Random walk\n",
    "fbm_persistent = generator.generate_fbm(n=1000, hurst=0.7)  # Persistent\n",
    "\n",
    "print(f\"Anti-persistent fBm (H=0.3): {len(fbm_anti)} points\")\n",
    "print(f\"Random walk fBm (H=0.5): {len(fbm_random)} points\")\n",
    "print(f\"Persistent fBm (H=0.7): {len(fbm_persistent)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd02ff8-846d-4c3e-81f8-eb0ae1f3c883",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `n`: Number of data points\n",
    "- `hurst`: Hurst exponent (0 < H < 1)\n",
    "- `random_state`: For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74622f-9f58-4dde-bd88-11c4ebb658d3",
   "metadata": {},
   "source": [
    "### Fractional Gaussian Noise (fGn)\n",
    "\n",
    "fGn represents the increments of fBm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e289ec-3f22-4515-8654-74772f73fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti-persistent fGn (H=0.3): 1000 points\n",
      "Random fGn (H=0.5): 1000 points\n",
      "Persistent fGn (H=0.7): 1000 points\n"
     ]
    }
   ],
   "source": [
    "# Generate fGn with different Hurst exponents using the new convenience method\n",
    "fgn_anti = generator.generate_fgn(n=1000, hurst=0.3)\n",
    "fgn_random = generator.generate_fgn(n=1000, hurst=0.5)\n",
    "fgn_persistent = generator.generate_fgn(n=1000, hurst=0.7)\n",
    "\n",
    "print(f\"Anti-persistent fGn (H=0.3): {len(fgn_anti)} points\")\n",
    "print(f\"Random fGn (H=0.5): {len(fgn_random)} points\")\n",
    "print(f\"Persistent fGn (H=0.7): {len(fgn_persistent)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc67cb22-cb08-4e29-b1e5-727b7c28cd26",
   "metadata": {},
   "source": [
    "## Data Contaminators\n",
    "### Polynomial Trends\n",
    "\n",
    "Add polynomial trends to simulate real-world non-stationarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d847341-3987-474e-82d5-4fdce33fa67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original signal variance: 0.0011\n",
      "Linear trend variance: 0.0011\n",
      "Quadratic trend variance: 0.0011\n"
     ]
    }
   ],
   "source": [
    "from data_processing.synthetic_generator import DataContaminator\n",
    "\n",
    "# Initialize contaminator\n",
    "contaminator = DataContaminator(random_state=42)\n",
    "\n",
    "# Add different polynomial trends\n",
    "linear_trend = contaminator.add_polynomial_trend(arfima_medium, degree=1, amplitude=0.1)\n",
    "quadratic_trend = contaminator.add_polynomial_trend(arfima_medium, degree=2, amplitude=0.05)\n",
    "cubic_trend = contaminator.add_polynomial_trend(arfima_medium, degree=3, amplitude=0.02)\n",
    "\n",
    "print(f\"Original signal variance: {np.var(arfima_medium):.4f}\")\n",
    "print(f\"Linear trend variance: {np.var(linear_trend):.4f}\")\n",
    "print(f\"Quadratic trend variance: {np.var(quadratic_trend):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b806a1-c93b-45a4-9be2-c4b2ec2d1036",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `signal`: Input time series\n",
    "- `degree`: Polynomial degree (1=linear, 2=quadratic, etc.)\n",
    "- `amplitude`: Trend strength relative to signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37430ee6-e66a-48b7-a5e4-c57d76a46adf",
   "metadata": {},
   "source": [
    "### Periodicity\n",
    "\n",
    "Add periodic components to simulate seasonal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d929b79a-052e-49eb-baaa-799fde894393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periodic signal variance: 0.0011\n",
      "Seasonal signal variance: 0.0011\n"
     ]
    }
   ],
   "source": [
    "# Add periodicity (note: frequency is a positional argument, not keyword)\n",
    "periodic_signal = contaminator.add_periodicity(arfima_medium, 50, amplitude=0.2)\n",
    "seasonal_signal = contaminator.add_periodicity(arfima_medium, 100, amplitude=0.15)\n",
    "\n",
    "print(f\"Periodic signal variance: {np.var(periodic_signal):.4f}\")\n",
    "print(f\"Seasonal signal variance: {np.var(seasonal_signal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83ef735-49be-4fad-bf5b-291a575df8de",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `signal`: Input time series\n",
    "- `frequency`: Period length (number of points)\n",
    "- `amplitude`: Periodic component strength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd9fe1-dcc8-4298-8c46-d73a6f335be0",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Add outliers to test robustness of analysis methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "013a0386-6684-455a-9381-80b93edb696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier signal variance: 0.0013\n",
      "Spike signal variance: 0.0012\n"
     ]
    }
   ],
   "source": [
    "# Add different types of outliers\n",
    "outlier_signal = contaminator.add_outliers(arfima_medium, fraction=0.02, magnitude=4.0)\n",
    "spike_signal = contaminator.add_outliers(arfima_medium, fraction=0.01, magnitude=6.0)\n",
    "\n",
    "print(f\"Outlier signal variance: {np.var(outlier_signal):.4f}\")\n",
    "print(f\"Spike signal variance: {np.var(spike_signal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556e531-3adc-4ec9-a11e-1cf3ed36a231",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `signal`: Input time series\n",
    "- `fraction`: Proportion of points to convert to outliers\n",
    "- `magnitude`: Outlier strength in standard deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4bad34-332a-4440-be8c-99b8a5fda602",
   "metadata": {},
   "source": [
    "### Heavy Tails\n",
    "\n",
    "Add heavy-tailed noise for non-Gaussian processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68f80823-327b-47cb-a5bc-39a185586237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heavy tail signal variance: 0.0011\n",
      "Cauchy signal variance: 0.0012\n"
     ]
    }
   ],
   "source": [
    "# Add heavy-tailed noise\n",
    "heavy_tail_signal = contaminator.add_heavy_tails(arfima_medium, df=2.0, fraction=0.15)\n",
    "cauchy_signal = contaminator.add_heavy_tails(arfima_medium, df=1.0, fraction=0.1)\n",
    "\n",
    "print(f\"Heavy tail signal variance: {np.var(heavy_tail_signal):.4f}\")\n",
    "print(f\"Cauchy signal variance: {np.var(cauchy_signal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39183212-d8df-4a5b-a5df-d2808317799a",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `signal`: Input time series\n",
    "- `df`: Degrees of freedom for t-distribution (lower = heavier tails)\n",
    "- `fraction`: Proportion of points to replace with heavy-tailed noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886004a-ee6d-4bd7-b858-df5fb68767a0",
   "metadata": {},
   "source": [
    "## Advanced Generation\n",
    "### Comprehensive Dataset Generation\n",
    "\n",
    "Generate a complete set of synthetic datasets for comprehensive testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f91d8ba-505b-48d3-9d3e-c52f9814aaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating comprehensive synthetic dataset...\n",
      "Saved synthetic data: data\\raw\\arfima_d0.1.csv\n",
      "Saved metadata: data\\metadata\\arfima_d0.1_metadata.json\n",
      "Saved synthetic data: data\\raw\\arfima_d0.2.csv\n",
      "Saved metadata: data\\metadata\\arfima_d0.2_metadata.json\n",
      "Saved synthetic data: data\\raw\\arfima_d0.3.csv\n",
      "Saved metadata: data\\metadata\\arfima_d0.3_metadata.json\n",
      "Saved synthetic data: data\\raw\\arfima_d0.4.csv\n",
      "Saved metadata: data\\metadata\\arfima_d0.4_metadata.json\n",
      "Saved synthetic data: data\\raw\\fbm_H0.3.csv\n",
      "Saved metadata: data\\metadata\\fbm_H0.3_metadata.json\n",
      "Saved synthetic data: data\\raw\\fbm_H0.5.csv\n",
      "Saved metadata: data\\metadata\\fbm_H0.5_metadata.json\n",
      "Saved synthetic data: data\\raw\\fbm_H0.7.csv\n",
      "Saved metadata: data\\metadata\\fbm_H0.7_metadata.json\n",
      "Saved synthetic data: data\\raw\\fgn_H0.3.csv\n",
      "Saved metadata: data\\metadata\\fgn_H0.3_metadata.json\n",
      "Saved synthetic data: data\\raw\\fgn_H0.5.csv\n",
      "Saved metadata: data\\metadata\\fgn_H0.5_metadata.json\n",
      "Saved synthetic data: data\\raw\\fgn_H0.7.csv\n",
      "Saved metadata: data\\metadata\\fgn_H0.7_metadata.json\n",
      "Saved synthetic data: data\\raw\\arfima_d03_trend.csv\n",
      "Saved metadata: data\\metadata\\arfima_d03_trend_metadata.json\n",
      "Saved synthetic data: data\\raw\\arfima_d03_periodic.csv\n",
      "Saved metadata: data\\metadata\\arfima_d03_periodic_metadata.json\n",
      "Saved synthetic data: data\\raw\\arfima_d03_outliers.csv\n",
      "Saved metadata: data\\metadata\\arfima_d03_outliers_metadata.json\n",
      "Saved synthetic data: data\\raw\\arfima_d03_heavy_tails.csv\n",
      "Saved metadata: data\\metadata\\arfima_d03_heavy_tails_metadata.json\n",
      "Saved synthetic data: data\\raw\\arfima_d03_combined.csv\n",
      "Saved metadata: data\\metadata\\arfima_d03_combined_metadata.json\n",
      "Saved synthetic data: data\\raw\\fbm_H05_trend.csv\n",
      "Saved metadata: data\\metadata\\fbm_H05_trend_metadata.json\n",
      "Saved synthetic data: data\\raw\\fbm_H05_periodic.csv\n",
      "Saved metadata: data\\metadata\\fbm_H05_periodic_metadata.json\n",
      "Saved synthetic data: data\\raw\\fbm_H05_outliers.csv\n",
      "Saved metadata: data\\metadata\\fbm_H05_outliers_metadata.json\n",
      "Saved synthetic data: data\\raw\\fbm_H05_heavy_tails.csv\n",
      "Saved metadata: data\\metadata\\fbm_H05_heavy_tails_metadata.json\n",
      "Saved synthetic data: data\\raw\\fbm_H05_combined.csv\n",
      "Saved metadata: data\\metadata\\fbm_H05_combined_metadata.json\n",
      "Saved synthetic data: data\\raw\\fgn_H07_trend.csv\n",
      "Saved metadata: data\\metadata\\fgn_H07_trend_metadata.json\n",
      "Saved synthetic data: data\\raw\\fgn_H07_periodic.csv\n",
      "Saved metadata: data\\metadata\\fgn_H07_periodic_metadata.json\n",
      "Saved synthetic data: data\\raw\\fgn_H07_outliers.csv\n",
      "Saved metadata: data\\metadata\\fgn_H07_outliers_metadata.json\n",
      "Saved synthetic data: data\\raw\\fgn_H07_heavy_tails.csv\n",
      "Saved metadata: data\\metadata\\fgn_H07_heavy_tails_metadata.json\n",
      "Saved synthetic data: data\\raw\\fgn_H07_combined.csv\n",
      "Saved metadata: data\\metadata\\fgn_H07_combined_metadata.json\n",
      "Saved synthetic data: data\\raw\\arfima_d03_missing_10pct.csv\n",
      "Saved metadata: data\\metadata\\arfima_d03_missing_10pct_metadata.json\n",
      "Saved synthetic data: data\\raw\\arfima_d03_missing_20pct.csv\n",
      "Saved metadata: data\\metadata\\arfima_d03_missing_20pct_metadata.json\n",
      "Saved synthetic data: data\\raw\\arfima_d03_missing_30pct.csv\n",
      "Saved metadata: data\\metadata\\arfima_d03_missing_30pct_metadata.json\n",
      "Saved synthetic data: data\\raw\\fbm_H05_missing_10pct.csv\n",
      "Saved metadata: data\\metadata\\fbm_H05_missing_10pct_metadata.json\n",
      "Saved synthetic data: data\\raw\\fbm_H05_missing_20pct.csv\n",
      "Saved metadata: data\\metadata\\fbm_H05_missing_20pct_metadata.json\n",
      "Saved synthetic data: data\\raw\\fbm_H05_missing_30pct.csv\n",
      "Saved metadata: data\\metadata\\fbm_H05_missing_30pct_metadata.json\n",
      "Saved synthetic data: data\\raw\\fgn_H07_missing_10pct.csv\n",
      "Saved metadata: data\\metadata\\fgn_H07_missing_10pct_metadata.json\n",
      "Saved synthetic data: data\\raw\\fgn_H07_missing_20pct.csv\n",
      "Saved metadata: data\\metadata\\fgn_H07_missing_20pct_metadata.json\n",
      "Saved synthetic data: data\\raw\\fgn_H07_missing_30pct.csv\n",
      "Saved metadata: data\\metadata\\fgn_H07_missing_30pct_metadata.json\n",
      "Generated 10 clean signals\n",
      "Generated 15 contaminated signals\n",
      "Generated 9 irregularly sampled signals\n",
      "Generated datasets:\n",
      "Clean signals: 10\n",
      "Contaminated signals: 15\n",
      "Irregular signals: 9\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive dataset\n",
    "comprehensive_dataset = generator.generate_comprehensive_dataset(\n",
    "    n=1000,\n",
    "    save=True,\n",
    ")\n",
    "\n",
    "print(\"Generated datasets:\")\n",
    "print(f\"Clean signals: {len(comprehensive_dataset['clean_signals'])}\")\n",
    "print(f\"Contaminated signals: {len(comprehensive_dataset['contaminated_signals'])}\")\n",
    "print(f\"Irregular signals: {len(comprehensive_dataset['irregular_signals'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c9f76-0b72-4866-9fec-ff513e07e22e",
   "metadata": {},
   "source": [
    "**Note**: The `generate_comprehensive_dataset` method automatically saves data to the default data directory. If you need to specify a custom data root, you can initialize the `SyntheticDataGenerator` with a custom `data_root` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967e0fda-3c3c-4c67-a98c-35d3fc621d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured directory exists: custom_data\n",
      "Ensured directory exists: custom_data\\raw\n",
      "Ensured directory exists: custom_data\\processed\n",
      "Ensured directory exists: custom_data\\metadata\n",
      "Generating comprehensive synthetic dataset...\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d0.1.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d0.1_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d0.2.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d0.2_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d0.3.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d0.3_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d0.4.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d0.4_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fbm_H0.3.csv\n",
      "Saved metadata: custom_data\\metadata\\fbm_H0.3_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fbm_H0.5.csv\n",
      "Saved metadata: custom_data\\metadata\\fbm_H0.5_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fbm_H0.7.csv\n",
      "Saved metadata: custom_data\\metadata\\fbm_H0.7_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fgn_H0.3.csv\n",
      "Saved metadata: custom_data\\metadata\\fgn_H0.3_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fgn_H0.5.csv\n",
      "Saved metadata: custom_data\\metadata\\fgn_H0.5_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fgn_H0.7.csv\n",
      "Saved metadata: custom_data\\metadata\\fgn_H0.7_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d03_trend.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d03_trend_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d03_periodic.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d03_periodic_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d03_outliers.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d03_outliers_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d03_heavy_tails.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d03_heavy_tails_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d03_combined.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d03_combined_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fbm_H05_trend.csv\n",
      "Saved metadata: custom_data\\metadata\\fbm_H05_trend_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fbm_H05_periodic.csv\n",
      "Saved metadata: custom_data\\metadata\\fbm_H05_periodic_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fbm_H05_outliers.csv\n",
      "Saved metadata: custom_data\\metadata\\fbm_H05_outliers_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fbm_H05_heavy_tails.csv\n",
      "Saved metadata: custom_data\\metadata\\fbm_H05_heavy_tails_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fbm_H05_combined.csv\n",
      "Saved metadata: custom_data\\metadata\\fbm_H05_combined_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fgn_H07_trend.csv\n",
      "Saved metadata: custom_data\\metadata\\fgn_H07_trend_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fgn_H07_periodic.csv\n",
      "Saved metadata: custom_data\\metadata\\fgn_H07_periodic_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fgn_H07_outliers.csv\n",
      "Saved metadata: custom_data\\metadata\\fgn_H07_outliers_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fgn_H07_heavy_tails.csv\n",
      "Saved metadata: custom_data\\metadata\\fgn_H07_heavy_tails_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fgn_H07_combined.csv\n",
      "Saved metadata: custom_data\\metadata\\fgn_H07_combined_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d03_missing_10pct.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d03_missing_10pct_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d03_missing_20pct.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d03_missing_20pct_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\arfima_d03_missing_30pct.csv\n",
      "Saved metadata: custom_data\\metadata\\arfima_d03_missing_30pct_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fbm_H05_missing_10pct.csv\n",
      "Saved metadata: custom_data\\metadata\\fbm_H05_missing_10pct_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fbm_H05_missing_20pct.csv\n",
      "Saved metadata: custom_data\\metadata\\fbm_H05_missing_20pct_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fbm_H05_missing_30pct.csv\n",
      "Saved metadata: custom_data\\metadata\\fbm_H05_missing_30pct_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fgn_H07_missing_10pct.csv\n",
      "Saved metadata: custom_data\\metadata\\fgn_H07_missing_10pct_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fgn_H07_missing_20pct.csv\n",
      "Saved metadata: custom_data\\metadata\\fgn_H07_missing_20pct_metadata.json\n",
      "Saved synthetic data: custom_data\\raw\\fgn_H07_missing_30pct.csv\n",
      "Saved metadata: custom_data\\metadata\\fgn_H07_missing_30pct_metadata.json\n",
      "Generated 10 clean signals\n",
      "Generated 15 contaminated signals\n",
      "Generated 9 irregularly sampled signals\n"
     ]
    }
   ],
   "source": [
    "# Initialize with custom data root\n",
    "generator = SyntheticDataGenerator(data_root=\"custom_data\", random_state=42)\n",
    "\n",
    "# Generate comprehensive dataset\n",
    "comprehensive_dataset = generator.generate_comprehensive_dataset(n=1000, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e83af9-8e5a-4d1f-b23d-7186179e8cb9",
   "metadata": {},
   "source": [
    "### Custom Signal Generation\n",
    "\n",
    "For more control, use the underlying pure generator directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dbf9b17-e282-4473-93cd-0d4cc67822e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom ARFIMA: 1000 points\n",
      "AR parameters: [0.3, -0.1]\n",
      "MA parameters: [0.2]\n"
     ]
    }
   ],
   "source": [
    "# Access the pure generator for advanced usage\n",
    "pure_generator = generator.pure_generator\n",
    "\n",
    "# Generate ARFIMA with custom parameters\n",
    "custom_arfima = pure_generator.generate_arfima(\n",
    "    n=1000, \n",
    "    d=0.25, \n",
    "    ar_params=[0.3, -0.1], \n",
    "    ma_params=[0.2], \n",
    "    sigma=0.8\n",
    ")\n",
    "\n",
    "print(f\"Custom ARFIMA: {len(custom_arfima)} points\")\n",
    "print(f\"AR parameters: [0.3, -0.1]\")\n",
    "print(f\"MA parameters: [0.2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14510cf4-fb05-4941-9c3d-7bf44b3efaab",
   "metadata": {},
   "source": [
    "## Data Quality and Validation\n",
    "### Signal Properties\n",
    "\n",
    "Check the statistical properties of generated signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc8e6b33-191b-480c-af49-908060572095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ARFIMA (d=0.3) Properties:\n",
      "  Length: 1000\n",
      "  Mean: 0.0000\n",
      "  Std: 0.0327\n",
      "  Min: -0.1189\n",
      "  Max: 0.0889\n",
      "  Variance: 0.0011\n",
      "\n",
      "fBm (H=0.7) Properties:\n",
      "  Length: 1000\n",
      "  Mean: 5.6135\n",
      "  Std: 5.6212\n",
      "  Min: -6.8601\n",
      "  Max: 14.5699\n",
      "  Variance: 31.5976\n",
      "\n",
      "fGn (H=0.6) Properties:\n",
      "  Length: 1000\n",
      "  Mean: -0.0055\n",
      "  Std: 0.0221\n",
      "  Min: -0.0828\n",
      "  Max: 0.0611\n",
      "  Variance: 0.0005\n",
      "\n",
      "Contaminated Properties:\n",
      "  Length: 1000\n",
      "  Mean: 0.0009\n",
      "  Std: 0.0352\n",
      "  Min: -0.1669\n",
      "  Max: 0.2502\n",
      "  Variance: 0.0012\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# First, let's create a contaminated signal for comparison\n",
    "from data_processing.synthetic_generator import DataContaminator\n",
    "\n",
    "# Initialize contaminator\n",
    "contaminator = DataContaminator(random_state=42)\n",
    "\n",
    "# Create a contaminated version of our ARFIMA signal\n",
    "contaminated_signal = contaminator.add_polynomial_trend(arfima_medium, degree=1, amplitude=0.1)\n",
    "contaminated_signal = contaminator.add_periodicity(contaminated_signal, 50, amplitude=0.2)\n",
    "contaminated_signal = contaminator.add_outliers(contaminated_signal, fraction=0.02, magnitude=3.0)\n",
    "\n",
    "def analyze_signal_properties(signal, name):\n",
    "    \"\"\"Analyze basic properties of a generated signal.\"\"\"\n",
    "    print(f\"\\n{name} Properties:\")\n",
    "    print(f\"  Length: {len(signal)}\")\n",
    "    print(f\"  Mean: {np.mean(signal):.4f}\")\n",
    "    print(f\"  Std: {np.std(signal):.4f}\")\n",
    "    print(f\"  Min: {np.min(signal):.4f}\")\n",
    "    print(f\"  Max: {np.max(signal):.4f}\")\n",
    "    print(f\"  Variance: {np.var(signal):.4f}\")\n",
    "\n",
    "# Analyze different signal types\n",
    "signals = {\n",
    "    \"ARFIMA (d=0.3)\": arfima_medium,\n",
    "    \"fBm (H=0.7)\": fbm_persistent,\n",
    "    \"fGn (H=0.6)\": fgn_persistent,\n",
    "    \"Contaminated\": contaminated_signal\n",
    "}\n",
    "\n",
    "for name, signal in signals.items():\n",
    "    analyze_signal_properties(signal, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eabcaf-16a5-4d16-b597-cbc3e16df42b",
   "metadata": {},
   "source": [
    "### Long-Range Dependence Validation\n",
    "\n",
    "Verify that generated signals exhibit the expected long-range dependence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57228392-e0cd-4747-993f-d694d01240f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ARFIMA (d=0.3) LRD Validation:\n",
      "  DFA Alpha: 0.720\n",
      "  DFA Hurst (H = α/2): 0.360\n",
      "  R/S Hurst: 0.764\n",
      "  ⚠ Hurst estimates differ (diff: 0.404)\n",
      "\n",
      "fBm (H=0.7) LRD Validation:\n",
      "  DFA Alpha: 1.669\n",
      "  DFA Hurst (H = α/2): 0.835\n",
      "  R/S Hurst: 1.011\n",
      "  ⚠ Hurst estimates differ (diff: 0.177)\n",
      "\n",
      "fGn (H=0.6) LRD Validation:\n",
      "  DFA Alpha: 0.738\n",
      "  DFA Hurst (H = α/2): 0.369\n",
      "  R/S Hurst: 0.755\n",
      "  ⚠ Hurst estimates differ (diff: 0.386)\n",
      "\n",
      "Contaminated LRD Validation:\n",
      "  DFA Alpha: 0.709\n",
      "  DFA Hurst (H = α/2): 0.354\n",
      "  R/S Hurst: 0.755\n",
      "  ⚠ Hurst estimates differ (diff: 0.400)\n"
     ]
    }
   ],
   "source": [
    "from analysis.dfa_analysis import dfa\n",
    "from analysis.rs_analysis import rs_analysis\n",
    "\n",
    "def validate_lrd(signal, name):\n",
    "    \"\"\"Validate long-range dependence properties.\"\"\"\n",
    "    print(f\"\\n{name} LRD Validation:\")\n",
    "    \n",
    "    try:\n",
    "        # DFA analysis\n",
    "        scales, flucts, dfa_summary = dfa(signal, order=1)\n",
    "        # DFA gives alpha, convert to Hurst: H = alpha/2\n",
    "        dfa_hurst = dfa_summary.alpha / 2\n",
    "        print(f\"  DFA Alpha: {dfa_summary.alpha:.3f}\")\n",
    "        print(f\"  DFA Hurst (H = α/2): {dfa_hurst:.3f}\")\n",
    "        \n",
    "        # R/S analysis\n",
    "        scales_rs, rs_values, rs_summary = rs_analysis(signal)\n",
    "        print(f\"  R/S Hurst: {rs_summary.hurst:.3f}\")\n",
    "        \n",
    "        # Check consistency between DFA and R/S\n",
    "        hurst_diff = abs(dfa_hurst - rs_summary.hurst)\n",
    "        if hurst_diff < 0.1:\n",
    "            print(f\"  ✓ Hurst estimates consistent (diff: {hurst_diff:.3f})\")\n",
    "        else:\n",
    "            print(f\"  ⚠ Hurst estimates differ (diff: {hurst_diff:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Analysis failed: {e}\")\n",
    "\n",
    "# Validate all signals\n",
    "for name, signal in signals.items():\n",
    "    validate_lrd(signal, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd2351-b637-4fd0-a036-2c8fb74ac08f",
   "metadata": {},
   "source": [
    "**Important Note**: Different analysis methods return different measures:\n",
    "\n",
    "- **DFA**: Returns `alpha` (scaling exponent), where Hurst exponent H = α/2\n",
    "- **R/S**: Returns `hurst` directly (Hurst exponent)\n",
    "- **MFDFA**: Returns `hq` array (generalized Hurst exponents for different q values)\n",
    "- **Wavelet**: Returns `hurst` directly (Hurst exponent)\n",
    "- **Spectral**: Returns `hurst` directly (Hurst exponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195f1293-a57c-463c-9cce-044d13eab146",
   "metadata": {},
   "source": [
    "## Data Storage and Management\n",
    "### Saving Generated Data\n",
    "\n",
    "Save generated datasets for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b8c93b2-8998-4905-bae2-3a2f1738f582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save individual signals\n",
    "np.save(\"data/raw/arfima_medium.npy\", arfima_medium)\n",
    "np.save(\"data/raw/fbm_persistent.npy\", fbm_persistent)\n",
    "\n",
    "# Save comprehensive dataset\n",
    "import pickle\n",
    "with open(\"data/raw/comprehensive_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(comprehensive_dataset, f)\n",
    "\n",
    "print(\"Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310136d7-cf65-4d6b-a8c0-047f52781c2c",
   "metadata": {},
   "source": [
    "### Loading Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "257d57b5-d6f2-417c-b38c-297e8d7effd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ARFIMA: 1000 points\n",
      "Loaded comprehensive dataset: 10 clean signals\n"
     ]
    }
   ],
   "source": [
    "# Load individual signals\n",
    "loaded_arfima = np.load(\"data/raw/arfima_medium.npy\")\n",
    "loaded_fbm = np.load(\"data/raw/fbm_persistent.npy\")\n",
    "\n",
    "# Load comprehensive dataset\n",
    "with open(\"data/raw/comprehensive_dataset.pkl\", \"rb\") as f:\n",
    "    loaded_comprehensive = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded ARFIMA: {len(loaded_arfima)} points\")\n",
    "print(f\"Loaded comprehensive dataset: {len(loaded_comprehensive['clean_signals'])} clean signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb343b-df16-4e0d-b5f0-3e66440d5a77",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "### Reproducibility\n",
    "\n",
    "- Always set `random_state` for reproducible results\n",
    "- Document all generation parameters\n",
    "- Use version control for generation scripts\n",
    "\n",
    "### Data Quality\n",
    "\n",
    "- Generate sufficient data points (recommend ≥500)\n",
    "- Validate statistical properties\n",
    "- Test with different contamination levels\n",
    "\n",
    "### Performance\n",
    "\n",
    "- Use batch generation for large datasets\n",
    "- Save intermediate results\n",
    "- Monitor memory usage for very long series\n",
    "\n",
    "### Validation\n",
    "\n",
    "- Always validate generated signals with analysis methods\n",
    "- Compare with theoretical expectations\n",
    "- Test robustness with contaminated data\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**Issue**: Generated signals don't show expected LRD\n",
    "**Solution**: Check parameter ranges and ensure sufficient data length\n",
    "\n",
    "**Issue**: Memory errors with large datasets\n",
    "**Solution**: Generate data in smaller batches or use streaming approaches\n",
    "\n",
    "**Issue**: Inconsistent results between runs\n",
    "**Solution**: Ensure random_state is set and check for global state changes\n",
    "\n",
    "**Issue**: Contamination not visible\n",
    "**Solution**: Increase amplitude parameters and check signal-to-noise ratios\n",
    "\n",
    "**Issue**: `TypeError: ArmaProcess.generate_sample() got an unexpected keyword argument 'random_state'`\n",
    "**Solution**: This issue has been fixed in the latest version. The method now properly handles reproducibility by setting the numpy random seed before calling `generate_sample()`. If you encounter this error, please update to the latest version.\n",
    "\n",
    "**Issue**: `TypeError: generate_comprehensive_dataset() got an unexpected keyword argument 'data_root'`\n",
    "**Solution**: The `generate_comprehensive_dataset()` method doesn't accept a `data_root` parameter. Use the constructor to set the data root: `SyntheticDataGenerator(data_root=\"custom_path\", random_state=42)`.\n",
    "\n",
    "### Recent Fixes Applied\n",
    "\n",
    "The following issues have been resolved in recent updates:\n",
    "\n",
    "1. **ArmaProcess Parameter Error**: Fixed `random_state` parameter issue in ARFIMA generation\n",
    "2. **Method Parameter Validation**: Corrected parameter lists for all generation methods\n",
    "3. **Import Path Updates**: Updated all import statements to match current codebase structure\n",
    "4. **Tutorial Accuracy**: All code examples now work with the current implementation\n",
    "\n",
    "### Getting Help\n",
    "\n",
    "If you encounter issues not covered here:\n",
    "\n",
    "1. **Check the project documentation**\n",
    "2. **Review the API reference**\n",
    "3. **Run the demo scripts**: `python scripts/demo_synthetic_data.py`\n",
    "4. **Create an issue on GitHub** with:\n",
    "    - Error message and traceback\n",
    "    - Code that caused the error\n",
    "    - Your system information (Python version, OS)\n",
    "    - Expected vs. actual behaviour\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Tutorial 3**: Learn advanced analysis methods\n",
    "- **Tutorial 4**: Understand statistical validation techniques\n",
    "- **Tutorial 5**: Create comprehensive visualizations\n",
    "- **Tutorial 6**: Submit your own models and datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
