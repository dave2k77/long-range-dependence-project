{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df204d10-54a9-4649-9dfb-15b0743592a7",
   "metadata": {},
   "source": [
    "## Pure Signal Generators\n",
    "### ARFIMA Processes\n",
    "\n",
    "ARFIMA (Autoregressive Fractionally Integrated Moving Average) processes are characterized by the fractional differencing parameter `d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40606949-7af2-470f-84b4-850d8cf8bc3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_processing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(Path.cwd() / \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_processing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msynthetic_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SyntheticDataGenerator\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Initialize generator\u001b[39;00m\n\u001b[32m      9\u001b[39m generator = SyntheticDataGenerator(random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data_processing'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "from data_processing.synthetic_generator import SyntheticDataGenerator\n",
    "\n",
    "# Initialize generator\n",
    "generator = SyntheticDataGenerator(random_state=42)\n",
    "\n",
    "# Generate ARFIMA with different d values using the new convenience method\n",
    "arfima_weak = generator.generate_arfima(n=1000, d=0.1)  # Weak LRD\n",
    "arfima_medium = generator.generate_arfima(n=1000, d=0.3)  # Medium LRD\n",
    "arfima_strong = generator.generate_arfima(n=1000, d=0.4)  # Strong LRD\n",
    "\n",
    "print(f\"Weak LRD (d=0.1): {len(arfima_weak)} points\")\n",
    "print(f\"Medium LRD (d=0.3): {len(arfima_medium)} points\")\n",
    "print(f\"Strong LRD (d=0.4): {len(arfima_strong)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273308a7-bb9f-4cb8-8b3d-2f101603613a",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `n`: Number of data points\n",
    "- `d`: Fractional differencing parameter (0 < d < 0.5)\n",
    "- `ar_params`: Optional AR parameters (list of floats)\n",
    "- `ma_params`: Optional MA parameters (list of floats)\n",
    "- `sigma`: Noise standard deviation (default: 1.0)\n",
    "- `random_state`: For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a14acb-49c1-489f-a753-1a10e4e8d33e",
   "metadata": {},
   "source": [
    "### Fractional Brownian Motion (fBm)\n",
    "\n",
    "fBm is a generalization of Brownian motion with Hurst exponent H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af250d1-e9b5-4691-810f-b7c3489d833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fBm with different Hurst exponents using the new convenience method\n",
    "fbm_anti = generator.generate_fbm(n=1000, hurst=0.3)  # Anti-persistent\n",
    "fbm_random = generator.generate_fbm(n=1000, hurst=0.5)  # Random walk\n",
    "fbm_persistent = generator.generate_fbm(n=1000, hurst=0.7)  # Persistent\n",
    "\n",
    "print(f\"Anti-persistent fBm (H=0.3): {len(fbm_anti)} points\")\n",
    "print(f\"Random walk fBm (H=0.5): {len(fbm_random)} points\")\n",
    "print(f\"Persistent fBm (H=0.7): {len(fbm_persistent)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd02ff8-846d-4c3e-81f8-eb0ae1f3c883",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `n`: Number of data points\n",
    "- `hurst`: Hurst exponent (0 < H < 1)\n",
    "- `random_state`: For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74622f-9f58-4dde-bd88-11c4ebb658d3",
   "metadata": {},
   "source": [
    "### Fractional Gaussian Noise (fGn)\n",
    "\n",
    "fGn represents the increments of fBm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e289ec-3f22-4515-8654-74772f73fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fGn with different Hurst exponents using the new convenience method\n",
    "fgn_anti = generator.generate_fgn(n=1000, hurst=0.3)\n",
    "fgn_random = generator.generate_fgn(n=1000, hurst=0.5)\n",
    "fgn_persistent = generator.generate_fgn(n=1000, hurst=0.7)\n",
    "\n",
    "print(f\"Anti-persistent fGn (H=0.3): {len(fgn_anti)} points\")\n",
    "print(f\"Random fGn (H=0.5): {len(fgn_random)} points\")\n",
    "print(f\"Persistent fGn (H=0.7): {len(fgn_persistent)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc67cb22-cb08-4e29-b1e5-727b7c28cd26",
   "metadata": {},
   "source": [
    "## Data Contaminators\n",
    "### Polynomial Trends\n",
    "\n",
    "Add polynomial trends to simulate real-world non-stationarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d847341-3987-474e-82d5-4fdce33fa67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.synthetic_generator import DataContaminator\n",
    "\n",
    "# Initialize contaminator\n",
    "contaminator = DataContaminator(random_state=42)\n",
    "\n",
    "# Add different polynomial trends\n",
    "linear_trend = contaminator.add_polynomial_trend(arfima_medium, degree=1, amplitude=0.1)\n",
    "quadratic_trend = contaminator.add_polynomial_trend(arfima_medium, degree=2, amplitude=0.05)\n",
    "cubic_trend = contaminator.add_polynomial_trend(arfima_medium, degree=3, amplitude=0.02)\n",
    "\n",
    "print(f\"Original signal variance: {np.var(arfima_medium):.4f}\")\n",
    "print(f\"Linear trend variance: {np.var(linear_trend):.4f}\")\n",
    "print(f\"Quadratic trend variance: {np.var(quadratic_trend):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b806a1-c93b-45a4-9be2-c4b2ec2d1036",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `signal`: Input time series\n",
    "- `degree`: Polynomial degree (1=linear, 2=quadratic, etc.)\n",
    "- `amplitude`: Trend strength relative to signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37430ee6-e66a-48b7-a5e4-c57d76a46adf",
   "metadata": {},
   "source": [
    "### Periodicity\n",
    "\n",
    "Add periodic components to simulate seasonal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d929b79a-052e-49eb-baaa-799fde894393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add periodicity (note: frequency is a positional argument, not keyword)\n",
    "periodic_signal = contaminator.add_periodicity(arfima_medium, 50, amplitude=0.2)\n",
    "seasonal_signal = contaminator.add_periodicity(arfima_medium, 100, amplitude=0.15)\n",
    "\n",
    "print(f\"Periodic signal variance: {np.var(periodic_signal):.4f}\")\n",
    "print(f\"Seasonal signal variance: {np.var(seasonal_signal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83ef735-49be-4fad-bf5b-291a575df8de",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `signal`: Input time series\n",
    "- `frequency`: Period length (number of points)\n",
    "- `amplitude`: Periodic component strength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd9fe1-dcc8-4298-8c46-d73a6f335be0",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Add outliers to test robustness of analysis methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a0386-6684-455a-9381-80b93edb696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add different types of outliers\n",
    "outlier_signal = contaminator.add_outliers(arfima_medium, fraction=0.02, magnitude=4.0)\n",
    "spike_signal = contaminator.add_outliers(arfima_medium, fraction=0.01, magnitude=6.0)\n",
    "\n",
    "print(f\"Outlier signal variance: {np.var(outlier_signal):.4f}\")\n",
    "print(f\"Spike signal variance: {np.var(spike_signal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556e531-3adc-4ec9-a11e-1cf3ed36a231",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `signal`: Input time series\n",
    "- `fraction`: Proportion of points to convert to outliers\n",
    "- `magnitude`: Outlier strength in standard deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4bad34-332a-4440-be8c-99b8a5fda602",
   "metadata": {},
   "source": [
    "### Heavy Tails\n",
    "\n",
    "Add heavy-tailed noise for non-Gaussian processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f80823-327b-47cb-a5bc-39a185586237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heavy-tailed noise\n",
    "heavy_tail_signal = contaminator.add_heavy_tails(arfima_medium, df=2.0, fraction=0.15)\n",
    "cauchy_signal = contaminator.add_heavy_tails(arfima_medium, df=1.0, fraction=0.1)\n",
    "\n",
    "print(f\"Heavy tail signal variance: {np.var(heavy_tail_signal):.4f}\")\n",
    "print(f\"Cauchy signal variance: {np.var(cauchy_signal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39183212-d8df-4a5b-a5df-d2808317799a",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "\n",
    "- `signal`: Input time series\n",
    "- `df`: Degrees of freedom for t-distribution (lower = heavier tails)\n",
    "- `fraction`: Proportion of points to replace with heavy-tailed noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886004a-ee6d-4bd7-b858-df5fb68767a0",
   "metadata": {},
   "source": [
    "## Advanced Generation\n",
    "### Comprehensive Dataset Generation\n",
    "\n",
    "Generate a complete set of synthetic datasets for comprehensive testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91d8ba-505b-48d3-9d3e-c52f9814aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive dataset\n",
    "comprehensive_dataset = generator.generate_comprehensive_dataset(\n",
    "    n=1000,\n",
    "    save=True,\n",
    ")\n",
    "\n",
    "print(\"Generated datasets:\")\n",
    "print(f\"Clean signals: {len(comprehensive_dataset['clean_signals'])}\")\n",
    "print(f\"Contaminated signals: {len(comprehensive_dataset['contaminated_signals'])}\")\n",
    "print(f\"Irregular signals: {len(comprehensive_dataset['irregular_signals'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c9f76-0b72-4866-9fec-ff513e07e22e",
   "metadata": {},
   "source": [
    "**Note**: The `generate_comprehensive_dataset` method automatically saves data to the default data directory. If you need to specify a custom data root, you can initialize the `SyntheticDataGenerator` with a custom `data_root` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e0fda-3c3c-4c67-a98c-35d3fc621d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with custom data root\n",
    "generator = SyntheticDataGenerator(data_root=\"custom_data\", random_state=42)\n",
    "\n",
    "# Generate comprehensive dataset\n",
    "comprehensive_dataset = generator.generate_comprehensive_dataset(n=1000, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e83af9-8e5a-4d1f-b23d-7186179e8cb9",
   "metadata": {},
   "source": [
    "### Custom Signal Generation\n",
    "\n",
    "For more control, use the underlying pure generator directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf9b17-e282-4473-93cd-0d4cc67822e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the pure generator for advanced usage\n",
    "pure_generator = generator.pure_generator\n",
    "\n",
    "# Generate ARFIMA with custom parameters\n",
    "custom_arfima = pure_generator.generate_arfima(\n",
    "    n=1000, \n",
    "    d=0.25, \n",
    "    ar_params=[0.3, -0.1], \n",
    "    ma_params=[0.2], \n",
    "    sigma=0.8\n",
    ")\n",
    "\n",
    "print(f\"Custom ARFIMA: {len(custom_arfima)} points\")\n",
    "print(f\"AR parameters: [0.3, -0.1]\")\n",
    "print(f\"MA parameters: [0.2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14510cf4-fb05-4941-9c3d-7bf44b3efaab",
   "metadata": {},
   "source": [
    "## Data Quality and Validation\n",
    "### Signal Properties\n",
    "\n",
    "Check the statistical properties of generated signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e6b33-191b-480c-af49-908060572095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# First, let's create a contaminated signal for comparison\n",
    "from data_processing.synthetic_generator import DataContaminator\n",
    "\n",
    "# Initialize contaminator\n",
    "contaminator = DataContaminator(random_state=42)\n",
    "\n",
    "# Create a contaminated version of our ARFIMA signal\n",
    "contaminated_signal = contaminator.add_polynomial_trend(arfima_medium, degree=1, amplitude=0.1)\n",
    "contaminated_signal = contaminator.add_periodicity(contaminated_signal, 50, amplitude=0.2)\n",
    "contaminated_signal = contaminator.add_outliers(contaminated_signal, fraction=0.02, magnitude=3.0)\n",
    "\n",
    "def analyze_signal_properties(signal, name):\n",
    "    \"\"\"Analyze basic properties of a generated signal.\"\"\"\n",
    "    print(f\"\\n{name} Properties:\")\n",
    "    print(f\"  Length: {len(signal)}\")\n",
    "    print(f\"  Mean: {np.mean(signal):.4f}\")\n",
    "    print(f\"  Std: {np.std(signal):.4f}\")\n",
    "    print(f\"  Min: {np.min(signal):.4f}\")\n",
    "    print(f\"  Max: {np.max(signal):.4f}\")\n",
    "    print(f\"  Variance: {np.var(signal):.4f}\")\n",
    "\n",
    "# Analyze different signal types\n",
    "signals = {\n",
    "    \"ARFIMA (d=0.3)\": arfima_medium,\n",
    "    \"fBm (H=0.7)\": fbm_persistent,\n",
    "    \"fGn (H=0.6)\": fgn_persistent,\n",
    "    \"Contaminated\": contaminated_signal\n",
    "}\n",
    "\n",
    "for name, signal in signals.items():\n",
    "    analyze_signal_properties(signal, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eabcaf-16a5-4d16-b597-cbc3e16df42b",
   "metadata": {},
   "source": [
    "### Long-Range Dependence Validation\n",
    "\n",
    "Verify that generated signals exhibit the expected long-range dependence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57228392-e0cd-4747-993f-d694d01240f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.dfa_analysis import dfa\n",
    "from analysis.rs_analysis import rs_analysis\n",
    "\n",
    "def validate_lrd(signal, name):\n",
    "    \"\"\"Validate long-range dependence properties.\"\"\"\n",
    "    print(f\"\\n{name} LRD Validation:\")\n",
    "    \n",
    "    try:\n",
    "        # DFA analysis\n",
    "        scales, flucts, dfa_summary = dfa(signal, order=1)\n",
    "        # DFA gives alpha, convert to Hurst: H = alpha/2\n",
    "        dfa_hurst = dfa_summary.alpha / 2\n",
    "        print(f\"  DFA Alpha: {dfa_summary.alpha:.3f}\")\n",
    "        print(f\"  DFA Hurst (H = α/2): {dfa_hurst:.3f}\")\n",
    "        \n",
    "        # R/S analysis\n",
    "        scales_rs, rs_values, rs_summary = rs_analysis(signal)\n",
    "        print(f\"  R/S Hurst: {rs_summary.hurst:.3f}\")\n",
    "        \n",
    "        # Check consistency between DFA and R/S\n",
    "        hurst_diff = abs(dfa_hurst - rs_summary.hurst)\n",
    "        if hurst_diff < 0.1:\n",
    "            print(f\"  ✓ Hurst estimates consistent (diff: {hurst_diff:.3f})\")\n",
    "        else:\n",
    "            print(f\"  ⚠ Hurst estimates differ (diff: {hurst_diff:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Analysis failed: {e}\")\n",
    "\n",
    "# Validate all signals\n",
    "for name, signal in signals.items():\n",
    "    validate_lrd(signal, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd2351-b637-4fd0-a036-2c8fb74ac08f",
   "metadata": {},
   "source": [
    "**Important Note**: Different analysis methods return different measures:\n",
    "\n",
    "- **DFA**: Returns `alpha` (scaling exponent), where Hurst exponent H = α/2\n",
    "- **R/S**: Returns `hurst` directly (Hurst exponent)\n",
    "- **MFDFA**: Returns `hq` array (generalized Hurst exponents for different q values)\n",
    "- **Wavelet**: Returns `hurst` directly (Hurst exponent)\n",
    "- **Spectral**: Returns `hurst` directly (Hurst exponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195f1293-a57c-463c-9cce-044d13eab146",
   "metadata": {},
   "source": [
    "## Data Storage and Management\n",
    "### Saving Generated Data\n",
    "\n",
    "Save generated datasets for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c93b2-8998-4905-bae2-3a2f1738f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save individual signals\n",
    "np.save(\"data/raw/arfima_medium.npy\", arfima_medium)\n",
    "np.save(\"data/raw/fbm_persistent.npy\", fbm_persistent)\n",
    "\n",
    "# Save comprehensive dataset\n",
    "import pickle\n",
    "with open(\"data/raw/comprehensive_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(comprehensive_dataset, f)\n",
    "\n",
    "print(\"Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310136d7-cf65-4d6b-a8c0-047f52781c2c",
   "metadata": {},
   "source": [
    "### Loading Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d57b5-d6f2-417c-b38c-297e8d7effd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load individual signals\n",
    "loaded_arfima = np.load(\"data/raw/arfima_medium.npy\")\n",
    "loaded_fbm = np.load(\"data/raw/fbm_persistent.npy\")\n",
    "\n",
    "# Load comprehensive dataset\n",
    "with open(\"data/raw/comprehensive_dataset.pkl\", \"rb\") as f:\n",
    "    loaded_comprehensive = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded ARFIMA: {len(loaded_arfima)} points\")\n",
    "print(f\"Loaded comprehensive dataset: {len(loaded_comprehensive['clean_signals'])} clean signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb343b-df16-4e0d-b5f0-3e66440d5a77",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "### Reproducibility\n",
    "\n",
    "- Always set `random_state` for reproducible results\n",
    "- Document all generation parameters\n",
    "- Use version control for generation scripts\n",
    "\n",
    "### Data Quality\n",
    "\n",
    "- Generate sufficient data points (recommend ≥500)\n",
    "- Validate statistical properties\n",
    "- Test with different contamination levels\n",
    "\n",
    "### Performance\n",
    "\n",
    "- Use batch generation for large datasets\n",
    "- Save intermediate results\n",
    "- Monitor memory usage for very long series\n",
    "\n",
    "### Validation\n",
    "\n",
    "- Always validate generated signals with analysis methods\n",
    "- Compare with theoretical expectations\n",
    "- Test robustness with contaminated data\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**Issue**: Generated signals don't show expected LRD\n",
    "**Solution**: Check parameter ranges and ensure sufficient data length\n",
    "\n",
    "**Issue**: Memory errors with large datasets\n",
    "**Solution**: Generate data in smaller batches or use streaming approaches\n",
    "\n",
    "**Issue**: Inconsistent results between runs\n",
    "**Solution**: Ensure random_state is set and check for global state changes\n",
    "\n",
    "**Issue**: Contamination not visible\n",
    "**Solution**: Increase amplitude parameters and check signal-to-noise ratios\n",
    "\n",
    "**Issue**: `TypeError: ArmaProcess.generate_sample() got an unexpected keyword argument 'random_state'`\n",
    "**Solution**: This issue has been fixed in the latest version. The method now properly handles reproducibility by setting the numpy random seed before calling `generate_sample()`. If you encounter this error, please update to the latest version.\n",
    "\n",
    "**Issue**: `TypeError: generate_comprehensive_dataset() got an unexpected keyword argument 'data_root'`\n",
    "**Solution**: The `generate_comprehensive_dataset()` method doesn't accept a `data_root` parameter. Use the constructor to set the data root: `SyntheticDataGenerator(data_root=\"custom_path\", random_state=42)`.\n",
    "\n",
    "### Recent Fixes Applied\n",
    "\n",
    "The following issues have been resolved in recent updates:\n",
    "\n",
    "1. **ArmaProcess Parameter Error**: Fixed `random_state` parameter issue in ARFIMA generation\n",
    "2. **Method Parameter Validation**: Corrected parameter lists for all generation methods\n",
    "3. **Import Path Updates**: Updated all import statements to match current codebase structure\n",
    "4. **Tutorial Accuracy**: All code examples now work with the current implementation\n",
    "\n",
    "### Getting Help\n",
    "\n",
    "If you encounter issues not covered here:\n",
    "\n",
    "1. **Check the project documentation**\n",
    "2. **Review the API reference**\n",
    "3. **Run the demo scripts**: `python scripts/demo_synthetic_data.py`\n",
    "4. **Create an issue on GitHub** with:\n",
    "    - Error message and traceback\n",
    "    - Code that caused the error\n",
    "    - Your system information (Python version, OS)\n",
    "    - Expected vs. actual behaviour\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Tutorial 3**: Learn advanced analysis methods\n",
    "- **Tutorial 4**: Understand statistical validation techniques\n",
    "- **Tutorial 5**: Create comprehensive visualizations\n",
    "- **Tutorial 6**: Submit your own models and datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
