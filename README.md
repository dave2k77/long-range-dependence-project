# Long-Range Dependence Analysis Project

A comprehensive Python framework for analyzing long-range dependence in time series data, featuring advanced fractal analysis methods and synthetic data generation capabilities.

## ğŸ¯ Overview

This project provides a complete toolkit for long-range dependence (LRD) analysis, including:

- **Multiple Analysis Methods**: DFA, R/S Analysis, Higuchi Method, MFDFA, Spectral Analysis, Wavelet Analysis
- **Synthetic Data Generation**: ARFIMA, Fractional Brownian Motion, Fractional Gaussian Noise with contamination
- **Statistical Validation**: Bootstrap, Cross-validation, Monte Carlo simulations, Hypothesis testing
- **Comprehensive Visualization**: Time series plots, fractal analysis results, validation plots
- **Data Management**: Automated data processing, quality assessment, and metadata management

## ğŸš€ Key Features

### Analysis Methods
- **Detrended Fluctuation Analysis (DFA)**: Robust method for estimating Hurst exponent
- **R/S Analysis**: Rescaled range analysis for long-range dependence detection
- **Higuchi Method**: Fractal dimension estimation for time series
- **Multifractal Detrended Fluctuation Analysis (MFDFA)**: Advanced multifractal analysis
- **Spectral Analysis**: Periodogram and Whittle estimation methods
- **Wavelet Analysis**: Continuous and discrete wavelet transforms

### Synthetic Data Generation
- **Pure Signal Generators**: ARFIMA(p,d,q), fBm, fGn processes
- **Data Contaminators**: Polynomial trends, periodicity, outliers, irregular sampling, heavy-tail fluctuations
- **Comprehensive Testing**: 34 different signal types for method validation

### Statistical Validation
- **Bootstrap Analysis**: Confidence intervals and uncertainty quantification
- **Cross-validation**: Robustness assessment across different data segments
- **Monte Carlo Simulations**: Performance evaluation under controlled conditions
- **Hypothesis Testing**: Statistical significance testing for LRD detection

### Submission System
- **Model Submission**: Submit new estimator models with validation and testing
- **Dataset Submission**: Submit new datasets with quality assessment
- **Standards Compliance**: Automatic validation against quality standards
- **Integration Testing**: Full analysis pipeline integration
- **Registry Management**: Centralized model and dataset registry

### Development & CI/CD
- **Continuous Integration**: Automated testing with GitHub Actions
- **Code Quality**: Linting, formatting, and type checking
- **Security**: Automated vulnerability scanning
- **Containerization**: Docker support for development and deployment
- **Pre-commit Hooks**: Automated code quality enforcement

## ğŸ“ Project Structure

```
long-range-dependence-project/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ analysis/                 # Analysis methods
â”‚   â”œâ”€â”€ data_processing/          # Data handling and synthetic generation
â”‚   â”œâ”€â”€ submission/               # Model and dataset submission system
â”‚   â””â”€â”€ visualisation/            # Plotting and visualization
â”œâ”€â”€ scripts/                      # Execution scripts
â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”œâ”€â”€ tests/                        # Unit tests
â”œâ”€â”€ data/                         # Data storage
â”‚   â”œâ”€â”€ raw/                      # Raw data files
â”‚   â”œâ”€â”€ processed/                # Processed data
â”‚   â””â”€â”€ metadata/                 # Data documentation
â”œâ”€â”€ results/                      # Analysis results
â”‚   â”œâ”€â”€ figures/                  # Generated plots
â”‚   â”œâ”€â”€ tables/                   # Results tables
â”‚   â””â”€â”€ reports/                  # Analysis reports
â”œâ”€â”€ config/                       # Configuration files
â”œâ”€â”€ docs/                         # Documentation
â”œâ”€â”€ tutorials/                    # Tutorial guides
â””â”€â”€ manuscript/                   # Research manuscript
```

## ğŸ›  Installation

### Prerequisites
- Python 3.8 or higher
- Git

### Setup

1. **Clone the repository**:
   ```bash
   git clone https://github.com/dave2k77/long-range-dependence-project.git
   cd long-range-dependence-project
   ```

2. **Create virtual environment**:
   ```bash
   python -m venv fractal-env
   source fractal-env/bin/activate  # On Windows: fractal-env\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Verify installation**:
   ```bash
   python scripts/demo_synthetic_data.py
   ```

## ğŸ“– Usage

### Quick Start

```python
from src.data_processing import SyntheticDataGenerator
from src.analysis import DFAnalysis, RSAnalysis

# Generate synthetic test data
generator = SyntheticDataGenerator(random_state=42)
dataset = generator.generate_comprehensive_dataset(n=1000, save=True)

# Analyze data
dfa = DFAnalysis()
rs = RSAnalysis()

for name, signal in dataset['clean_signals'].items():
    dfa_result = dfa.analyze(signal)
    rs_result = rs.analyze(signal)
    print(f"{name}: DFA={dfa_result['hurst']:.3f}, RS={rs_result['hurst']:.3f}")
```

### Command Line Usage

**Generate synthetic data**:
```bash
# Generate comprehensive dataset
python scripts/generate_synthetic_data.py --n 1000

# Generate only clean signals
python scripts/generate_synthetic_data.py --clean-only --n 500
```

**Run analysis**:
```bash
# Run full analysis pipeline
python scripts/run_full_analysis.py

# Test specific methods
python scripts/test_analysis.py
```

**Demonstrations**:
```bash
# Synthetic data generation demo
python scripts/demo_synthetic_data.py

# ARFIMA modeling demo
python scripts/demo_arfima.py
```

### Jupyter Notebooks

- `notebooks/01_synthetic_data_generation.ipynb`: Synthetic data generation tutorial
- `notebooks/04_arfima_modelling.ipynb`: ARFIMA modeling examples
- `notebooks/05_dfa_analysis.ipynb`: DFA analysis tutorial

## ğŸ”¬ Analysis Methods

### Detrended Fluctuation Analysis (DFA)
```python
from src.analysis import DFAnalysis

dfa = DFAnalysis()
result = dfa.analyze(time_series)
print(f"Hurst exponent: {result['hurst']:.3f}")
```

### R/S Analysis
```python
from src.analysis import RSAnalysis

rs = RSAnalysis()
result = rs.analyze(time_series)
print(f"Hurst exponent: {result['hurst']:.3f}")
```

### Higuchi Method
```python
from src.analysis import HiguchiAnalysis

higuchi = HiguchiAnalysis()
result = higuchi.analyze(time_series)
print(f"Fractal dimension: {result['fractal_dimension']:.3f}")
```

### Multifractal Analysis
```python
from src.analysis import MFDFAAnalysis

mfdfa = MFDFAAnalysis()
result = mfdfa.analyze(time_series)
print(f"Multifractal spectrum: {result['spectrum']}")
```

## ğŸ“Š Synthetic Data Generation

### Pure Signal Generators
```python
from src.data_processing import PureSignalGenerator

generator = PureSignalGenerator(random_state=42)

# ARFIMA process
arfima_signal = generator.generate_arfima(n=1000, d=0.3)

# Fractional Brownian Motion
fbm_signal = generator.generate_fbm(n=1000, hurst=0.7)

# Fractional Gaussian Noise
fgn_signal = generator.generate_fgn(n=1000, hurst=0.5)
```

### Data Contamination
```python
from src.data_processing import DataContaminator

contaminator = DataContaminator(random_state=42)

# Add polynomial trend
trend_signal = contaminator.add_polynomial_trend(signal, degree=2, amplitude=0.1)

# Add periodicity
periodic_signal = contaminator.add_periodicity(signal, frequency=50, amplitude=0.2)

# Add outliers
outlier_signal = contaminator.add_outliers(signal, fraction=0.02, magnitude=4.0)
```

## ğŸ§ª Testing

Run the comprehensive test suite:
```bash
python -m pytest tests/ -v
```

Run specific test modules:
```bash
python -m pytest tests/test_synthetic_generator.py -v
python -m pytest tests/test_dfa.py -v
```

## ğŸ“ˆ Results and Visualization

The project automatically generates comprehensive visualizations:

- **Time Series Plots**: Raw and processed data visualization
- **Fractal Analysis Results**: DFA, R/S, Higuchi method plots
- **Validation Results**: Bootstrap, cross-validation, Monte Carlo plots
- **Synthetic Data Examples**: Pure and contaminated signal comparisons

## ğŸ”„ Submission System

### Model Submission

Submit new estimator models to the benchmark:

```python
from src.submission import SubmissionManager, ModelMetadata

# Create model metadata
metadata = ModelMetadata(
    name="MyEstimator",
    version="1.0.0",
    author="Your Name",
    description="Description of your model",
    category="custom",
    parameters={"param1": 1.0},
    dependencies=["numpy", "pandas"]
)

# Submit model
manager = SubmissionManager()
result = manager.submit_model(
    model_file="path/to/your/model.py",
    metadata=metadata,
    run_full_analysis=True
)

print(f"Submission ID: {result.submission_id}")
print(f"Status: {result.status.value}")
```

### Dataset Submission

Submit new datasets for analysis:

```python
from src.submission import DatasetMetadata

# Create dataset metadata
metadata = DatasetMetadata(
    name="MyDataset",
    version="1.0.0",
    author="Your Name",
    description="Description of your dataset",
    category="financial",
    source="Data source",
    sampling_frequency="1 hour",
    units="USD",
    collection_date="2024-01-01"
)

# Submit dataset
result = manager.submit_dataset(
    file_path="path/to/your/dataset.csv",
    metadata=metadata,
    run_full_analysis=True
)
```

### Standards and Validation

The submission system automatically validates:

- **Model Standards**: Interface compliance, performance thresholds, documentation
- **Dataset Standards**: Format requirements, quality metrics, metadata completeness
- **Integration Testing**: Full analysis pipeline compatibility
- **Performance Benchmarking**: Comparison with existing methods

For detailed information, see the [Submission System Tutorial](tutorials/06_submission_system.md).

Results are saved in the `results/` directory with organized subdirectories.

## ğŸš€ Development & CI/CD

### Quick Start

```bash
# Install development dependencies
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install

# Run all CI checks locally
make ci
```

### Available Commands

```bash
# Testing
make test              # Run all tests
make test-cov          # Run tests with coverage
make test-fast         # Run fast tests only

# Code Quality
make lint              # Run linting
make format            # Format code
make type-check        # Run type checking
make security          # Run security checks

# Development
make install-dev       # Install development dependencies
make clean             # Clean generated files
make docs              # Build documentation
```

### Docker Development

```bash
# Start all services
docker-compose up -d

# Run tests in container
docker-compose run test

# Start development shell
docker-compose run dev

# Start Jupyter notebook
docker-compose up jupyter
```

### CI/CD Pipeline

The project uses GitHub Actions for automated:

- **Multi-Python Testing**: Python 3.8, 3.9, 3.10, 3.11
- **Code Quality**: Linting, formatting, type checking
- **Security**: Vulnerability scanning
- **Coverage**: Test coverage reporting
- **Documentation**: Automated docs building

For detailed CI/CD information, see [CI/CD Guide](docs/ci_cd_guide.md).

## ğŸ“š Documentation

- `docs/synthetic_data_generation.md`: Synthetic data generation guide
- `docs/methodology.md`: Analysis method descriptions
- `docs/api_documentation.md`: API reference
- `docs/analysis_protocol.md`: Analysis workflow documentation
- `docs/ci_cd_guide.md`: CI/CD and development guide

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

The MIT License is a permissive license that allows for:
- âœ… Commercial use
- âœ… Modification
- âœ… Distribution
- âœ… Private use
- âœ… Patent use

**Requirements**: The only requirement is that the license and copyright notice be included in all copies or substantial portions of the software.

## ğŸ† Acknowledgments

- **ARFIMA processes**: Granger & Joyeux (1980)
- **Fractional Brownian Motion**: Mandelbrot & Van Ness (1968)
- **DFA method**: Peng et al. (1994)
- **R/S Analysis**: Hurst (1951), Mandelbrot (1971)
- **Higuchi Method**: Higuchi (1988)

## ğŸ“ Contact

For questions and support, please open an issue on GitHub or contact the maintainers.

---

**Note**: This project is designed for research purposes and provides a comprehensive framework for long-range dependence analysis. The synthetic data generation capabilities make it particularly useful for method validation and benchmarking.

